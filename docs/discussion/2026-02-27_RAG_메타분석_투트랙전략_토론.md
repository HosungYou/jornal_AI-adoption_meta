# RAG 기반 메타분석 투트랙 전략 토론 기록

> **일시**: 2026-02-27
> **참여**: Hosung You (PI) + Claude Opus 4.6 (AI Research Assistant)
> **맥락**: Paper B 구성을 위한 RAG + 메타분석 접근법 탐색
> **관련 리포지토리**: dissertation_AI-adoption_meta, jornal_AI-adoption_meta, ScholaRAG, scholar-graphrag, SLR-Automation-with-GPT-4, Diverga

---

## 1. 토론 배경

### 1.1 출발점

RAG(검색 증강 생성) 인덱싱이 메타분석 과정에 실질적으로 기여할 수 있는지, 특히 투트랙(Two-Track) 전략의 효과성을 측정하는 것을 Paper B의 연구 방향으로 잡을 수 있는지에 대한 탐색적 논의.

### 1.2 RAG가 메타분석에 기여할 수 있는 영역 (초기 제안)

- **초기 문헌 스크리닝 가속화**: 의미론적(Semantic) 검색으로 Boolean 검색 한계 극복
- **질적 데이터 및 조절변인 코딩**: 본문 속 맥락적 데이터 추출
- **이론적 맥락 통합**: 서론/논의 부분에서 인사이트 수집

### 1.3 RAG의 한계점 (초기 제안)

- **표(Table) 데이터 인식의 취약성**: 상관계수 행렬 등 2차원 표 구조 인덱싱 어려움
- **수치 데이터 환각(Hallucination) 리스크**: 효과크기 오추출 위험
- **재현성(Reproducibility) 한계**: 임베딩 모델 업데이트, 유사도 임계값 설정에 따른 결과 변동

### 1.4 투트랙 전략 초기 제안

- **Track 1 (의미론적 검색)**: 논문 섹션별 메타데이터 필터링 + 벡터 검색
- **Track 2 (구조화 표 파싱)**: LLM 기반 표 → Markdown/JSON 변환 → 정형화 DB
- **교차 검증 UI**: RAG 검색 결과의 원문 출처 링크 필수 구현

---

## 2. GitHub 리포지토리 자산 분석

### 2.1 Paper B와 직결되는 리포지토리

| 리포지토리 | Paper B에서의 역할 |
|---|---|
| `dissertation_AI-adoption_meta` | 메타분석 원 데이터셋 + 방법론적 경험 |
| `jornal_AI-adoption_meta` | Paper A/B 공동 리포지토리 |
| `ScholaRAG` / `ScholaRAG-helper` | RAG 기반 학술 검색 도구 (실증 시스템) |
| `scholar-graphrag` | GraphRAG 기반 관계 추론 |
| `SLR-Automation-with-GPT-4` | GPT-4 기반 체계적 문헌고찰 자동화 파일럿 |
| `Diverga` | 44개 연구 에이전트 (메타분석 파이프라인 C5/C6/C7/I0-I3) |

### 2.2 핵심 강점

PI가 (1) 메타분석 수행 경험, (2) RAG 시스템 구축 경험, (3) SLR 자동화 파일럿 경험을 모두 보유 → 단순 개념 제안이 아닌 실증적 비교 연구 가능.

---

## 3. 학술 문헌 검색 결과

### 3.1 RAG + 메타분석 직접 결합 시도

**Ahad et al. (2024)** — "Empowering Meta-Analysis: Leveraging Large Language Models for Scientific Synthesis"
- RAG + fine-tuned LLM + 프롬프트 엔지니어링으로 메타분석 초록 자동 생성
- 87.6% 관련성 달성, 비관련성 4.56% → 1.9% 감소
- **한계**: low-resource 환경 실험, 데이터 추출이 아닌 "합성(synthesis)" 초점, 효과크기 정확도 미측정
- 출처: arXiv 2411.10878 / IEEE Xplore

### 3.2 자동화 메타분석 체계적 리뷰

**PRISMA SR (2025)** — "Transforming Evidence Synthesis: A Systematic Review of the Evolution of Automated Meta-Analysis in the Age of AI"
- 978편 중 54편 분석 (2006-2024)
- 자동화의 57%가 데이터 추출/통계 모델링에 집중
- 17%만 고급 합성 단계, **완전한 end-to-end 자동화는 단 2%**
- 의학 67% vs 비의학 33%
- 출처: arXiv 2504.20113

### 3.3 LLM 메타분석 데이터 추출 벤치마크

**arXiv (2025)** — "What Level of Automation is 'Good Enough'?"
- Gemini-2.0-flash, Grok-3, GPT-4o-mini 3개 모델 비교
- 모든 모델이 높은 precision, **일관적으로 낮은 recall** (핵심 정보 누락)
- 커스텀 프롬프트가 recall 15% 향상 — 가장 효과적
- **3단계 자동화 가이드라인** 제안: 데이터 유형별 자동화 수준 차등 적용
- 출처: arXiv 2507.15152

### 3.4 RAG + 바이오메디컬 LLM 메타분석

**JAMIA (2025)** — "Improving large language model applications in biomedicine with RAG"
- RAG가 바이오메디컬 LLM 성능을 OR 1.35로 향상
- 335편 중 20편 포함
- 출처: JAMIA 32(4):605

### 3.5 보고 가이드라인

**PRISMA-trAIce (2025)** — AI 활용 체계적 문헌고찰의 투명한 보고를 위한 14개 항목 체크리스트
- 출처: JMIR AI 2025/1/e80247

### 3.6 출판 편향 탐지

**Cochrane (2025)** — LLM의 출판 편향 탐지 능력이 fine-tuning 없이는 제한적
- 출처: Cochrane Evidence Synthesis and Methods

### 3.7 증거 합성 자동화 도구 비교

- Covidence: 164개 연구에서 사용 (가장 많이 사용됨)
- ASReview: 스크리닝 82% 감소 효과
- RobotReviewer: NLP 기반, RCT 분류 오류 존재
- 출처: Cochrane/Campbell/JBI/CEE 2025 합동 리뷰

### 3.8 구조화 데이터 추출 벤치마크

- LLMStructBench (2025): 22개 모델, 5개 프롬프팅 전략 비교
- GPT-4o: 91.4% 정확도 달성
- LLM-TKIE: F1 80.9, 정확도 88.85 (CORD 데이터셋)
- 출처: arXiv 2602.14743v1

### 3.9 HRD/교육 분야 AI 채택

- 43% 조직이 HR에 AI 활용 (2024년 26% 대비 증가)
- AI 채택 메타합성: 5개 주제 식별 (조직/전략, 기술/운영, 인간중심, 도전/기회, 환경/경제)
- 출처: SHRM 2025, Emerald JWAM

---

## 4. 핵심 논의: 연구 포지셔닝

### 4.1 "언제/어떤 조건에서 효과적인가"를 밝히는 논문으로의 포지셔닝

**합의**: Paper B는 "RAG/LLM이 메타분석에 좋다"를 증명하는 논문이 아니라, "메타분석의 어떤 단계에서, 어떤 조건의 데이터에, 어떤 수준의 자동화가 적절한가"를 경험적으로 밝히는 논문으로 포지셔닝.

이것은 2025 arXiv 벤치마크의 3단계 자동화 가이드라인과 일맥상통.

### 4.2 ScholaRAG + Diverga 공개에 대한 논의

**PI 의견**: 너무 복잡할 것 같다.

**AI 분석 동의 근거**:
- ScholaRAG(검색 시스템) + Diverga(44개 에이전트 프레임워크) 문서화만으로 논문 1편 분량
- 리뷰어가 "시스템 논문인가, 방법론 논문인가" 혼란 가능
- HRD/교육학 저널 리뷰어가 RAG 시스템 아키텍처를 평가할 역량 부족 가능성

**대안: "도구 불가지론적(Tool-Agnostic)" 방법론 논문**:
- 핵심 기여 = "투트랙 전략" 자체의 효과성 ≠ 특정 도구의 성능
- Track 1: "벡터 기반 의미론적 검색" (구현체 무관)
- Track 2: "LLM 기반 표 파싱 → 정형 DB" (모델 교체 가능)
- ScholaRAG는 "실험에 사용된 구현체"로만 언급 (GitHub 링크 각주)

### 4.3 RAG + 메타분석 기존 시도 여부

**결론**: 있음. 하지만 gap이 명확함.

| 기존 연구 | 초점 | Paper B가 채울 gap |
|---|---|---|
| Ahad et al. (2024) | 메타분석 초록 "생성" | 효과크기 "추출 정확도" 미검증 |
| arXiv 벤치마크 (2025) | 의학 분야 3모델 비교 | 사회과학 맥락 부재 |
| Cochrane/Campbell 리뷰 | Covidence/ASReview 등 기존 도구 | RAG 기반 의미론적 검색 미적용 |
| PRISMA SR (2025) | end-to-end 자동화 2%만 | 단계별 효과성 미측정 |

**사회과학 분야에서 RAG의 메타분석 데이터 추출 정확도를 Gold Standard 대비 정량적으로 비교한 연구는 발견되지 않음.**

### 4.4 CS 확장선으로서의 접근

**PI 의견**: 이 접근은 CS 혹은 다른 분야에서 진행했을 것 같고, RAG과 데이터 추출의 정합성 등의 확장이라고 볼 수 있다.

**AI 분석 동의**:
- RAG + 메타분석은 CS의 정보 검색(IR) + 구조화 데이터 추출 연구의 확장선에 있음
- RAG 평가 메트릭(relevance, accuracy, faithfulness)은 메타분석 데이터 추출 평가에 직접 매핑 가능
- 하이브리드 검색(dense vector + BM25 sparse) 연구에서 15-30% precision 향상 보고

---

## 5. 방법론적 비교 연구 구성 논의

### 5.1 Gold Standard 비교법

**핵심 자산**: `dissertation_AI-adoption_meta` 데이터 — 수동 추출 결과가 "정답지(ground truth)"

**4개 조건 비교 설계**:
| 조건 | 입력 | 처리 방식 | 산출물 |
|---|---|---|---|
| A. 수동 (Gold Std) | 원문 PDF | 연구자 직접 코딩 | 코딩시트 (이미 존재) |
| B. Track 1만 | 원문 PDF → RAG 인덱싱 | 의미론적 검색 + LLM 질의 | 자동 코딩 결과 |
| C. Track 2만 | 원문 PDF → 표 파싱 | LLM 표→JSON + 정형DB | 자동 수치 추출 |
| D. Track 1+2 | 원문 PDF | 양 트랙 통합 | 통합 코딩 결과 |

### 5.2 비교 지표

- 1차: MAE, 완전일치율, 범위 오류율 (효과크기 추출 정확도)
- 2차: Precision, Recall, F1 (스크리닝 성능)
- 3차: 분/논문 소요 시간, 인적 개입 빈도 (효율성)
- 4차: Cohen's kappa, 누락률 (조절변인 코딩)

### 5.3 조건별 효과성 분석 ("언제/어떤 조건에서")

| 분석 질문 | 측정 방법 |
|---|---|
| 표가 복잡할수록 Track 2 정확도가 떨어지는가? | 표 복잡도(행×열) × MAE 상관 |
| 텍스트 기반 보고일 때 Track 1이 더 나은가? | 보고 형태별 조건 비교 |
| 논문 길이가 RAG 검색 정확도에 영향을 미치는가? | 페이지 수별 Precision/Recall |
| 어떤 유형의 효과크기(r, d, OR)에서 오류가 많은가? | 효과크기 유형별 MAE |

---

## 6. 반박 및 대응

### 반박 1: "이건 CS 논문이지, HRD 논문이 아니다"
- **대응**: "RAG 시스템 개발" 논문이 아닌 "메타분석 방법론의 효율화" 논문
- **타겟 저널**: Research Synthesis Methods (IF 9.0+) 또는 Organizational Research Methods
- **선례**: ASReview도 CS 도구이지만 Systematic Reviews에 게재

### 반박 2: "표본이 너무 작다"
- **대응**: "메타분석을 하는 연구"가 아닌 "메타분석 방법을 비교하는 연구"
- 50-100편도 충분한 비교 단위 (기존 벤치마크도 유사 규모)

### 반박 3: "임베딩 모델이 바뀌면 결과가 달라진다 — 재현 불가"
- **대응**: PRISMA-trAIce 14개 항목 적용 + 임베딩 모델/버전/threshold 명시 기록
- 민감도 분석으로 threshold 변화에 따른 결과 변동 보고
- limitation으로 명시하되, "향후 표준화 연구의 필요성을 지지한다"로 전환

### 반박 4: "Covidence + 수동이 더 정확하면 투트랙의 의미가 없다"
- **대응**: "수동이 더 정확하다"도 학술적 기여 (부정적 결과 보고)
- 핵심은 정확도 vs 시간의 trade-off 곡선
- 95% 정확도를 1/5 시간에 달성한다면 그 자체가 의미 있음

### 반박 5: "HRD 분야에서 방법론 논문이 게재 가능한가"
- **대응**: HRDR, IJTD 등에서 방법론적 혁신 논문 수용
- SHRM 데이터: 43% 조직이 HR에 AI 활용 중으로 시의성 높음

---

## 7. 결론 및 합의

### 7.1 연구 방향 타당성

**강력히 지지.** 다음 조건에서:
1. "RAG가 좋다" 증명이 아닌 "언제/어떤 조건에서 효과적인가"를 밝히는 포지셔닝
2. dissertation 데이터를 Gold Standard로 재활용 — 방법론적 비교 연구
3. 도구 불가지론적 접근 — ScholaRAG/Diverga는 구현체로만 언급
4. PRISMA-trAIce 최초 적용 HRD 분야 사례 — 선점 효과

### 7.2 가장 큰 차별화 포인트

**사회과학/HRD 맥락에서 RAG 기반 메타분석 자동화를 실증한 연구가 사실상 부재** — 이것이 Paper B의 학술적 기여.

### 7.3 Paper B 현재 설계와의 관계

Paper B는 이미 LLM 3모델 비교 + Gold Standard + 4개 RQ로 견고한 프레임워크를 갖추고 있음.
토론에서 제안된 RAG 투트랙 전략은 Paper B의 기존 Module B (Statistical: correlation matrix 추출)와 Module A/C/D (text-based) 구분에 자연스럽게 매핑됨.

**핵심 제안**: Paper B의 기존 설계를 "투트랙" 프레이밍으로 재해석하여, 문헌 리뷰에 RAG+메타분석 맥락을 추가하고, RQ2의 Variable Type별 정확도 차이 분석을 투트랙 전략의 실증적 근거로 포지셔닝.

---

## 8. 참고 문헌

- Ahad, J., et al. (2024). Empowering Meta-Analysis: Leveraging Large Language Models for Scientific Synthesis. arXiv:2411.10878 / IEEE Xplore.
- JAMIA (2025). Improving large language model applications in biomedicine with RAG. JAMIA 32(4):605.
- arXiv (2025). What Level of Automation is "Good Enough"? arXiv:2507.15152.
- PRISMA-trAIce Consortium (2025). Transparent Reporting of AI in Systematic Literature Reviews. JMIR AI 2025/1/e80247.
- Xing (2025). Leveraging AI for Meta-Analysis: Evaluating LLMs in Detecting Publication Bias. Cochrane Evidence Synthesis and Methods.
- Scotti (2025). AI and Automation in Evidence Synthesis. Cochrane Evidence Synthesis and Methods.
- arXiv (2025). Transforming Evidence Synthesis: A Systematic Review of the Evolution of Automated Meta-Analysis in the Age of AI. arXiv:2504.20113.
- LLMStructBench (2025). Benchmarking LLM Structured Data Extraction. arXiv:2602.14743.
- SHRM (2025). The Role of AI in HR Continues to Expand.
- Emerald JWAM (2025). Factors influencing AI adoption in HRM: meta-synthesis.
