# Construct Crosswalk: TAM, UTAUT, and AI-Specific Frameworks

**Complete Mapping Table for 12 Target Constructs**

**Purpose:** This crosswalk guides construct harmonization during data extraction. Use this to map study-specific labels to standardized constructs.

---

## Complete Crosswalk Table

| Standard Construct | TAM (Davis, 1989) | TAM2 (Venkatesh & Davis, 2000) | TAM3 (Venkatesh & Bala, 2008) | UTAUT (Venkatesh et al., 2003) | UTAUT2 (Venkatesh et al., 2012) | TPB (Ajzen, 1991) | AI-Specific Literature | Common Aliases & Variants |
|---|---|---|---|---|---|---|---|---|
| **PE** (Performance Expectancy) | Perceived Usefulness (PU) | Perceived Usefulness (PU) | Perceived Usefulness (PU) | Performance Expectancy (PE) | Performance Expectancy (PE) | Attitude toward Behavior (partial) | AI Usefulness, AI Performance | Expected Performance, Outcome Expectations, Job Performance, Productivity Gain, Relative Advantage (IDT), Extrinsic Motivation |
| **EE** (Effort Expectancy) | Perceived Ease of Use (PEOU) | Perceived Ease of Use (PEOU) | Perceived Ease of Use (PEOU) | Effort Expectancy (EE) | Effort Expectancy (EE) | Perceived Behavioral Control (partial) | AI Ease of Use, User-Friendliness | Complexity (reverse), Ease of Learning, Learnability, Cognitive Effort (reverse), Usability |
| **SI** (Social Influence) | — (not in TAM) | Subjective Norm (SN), Image | Subjective Norm (SN), Image | Social Influence (SI) | Social Influence (SI) | Subjective Norm (SN) | Social Pressure (AI), Peer Influence | Normative Beliefs, Social Norms, Social Factors, Peer Pressure, External Influence |
| **FC** (Facilitating Conditions) | — (not in TAM) | — (not in TAM2) | — (not in TAM3) | Facilitating Conditions (FC) | Facilitating Conditions (FC) | Perceived Behavioral Control (partial) | Technical Support, Organizational Support | Resource Availability, Compatibility, Organizational Facilitators, Infrastructure Support |
| **BI** (Behavioral Intention) | Behavioral Intention to Use (BI) | Behavioral Intention (BI) | Behavioral Intention (BI) | Behavioral Intention (BI) | Behavioral Intention (BI) | Intention | Intention to Use AI | Adoption Intention, Usage Intention, Continuance Intention, Willingness to Use, Acceptance Intention |
| **UB** (Use Behavior) | System Use, Actual Usage | Actual System Use | Actual System Use | Use Behavior (UB) | Use Behavior (UB) | Behavior | AI Usage, AI Adoption | Actual Use, Usage Frequency, Usage Duration, System Utilization, Adoption Behavior, Continuance Use |
| **ATT** (Attitude) | Attitude Toward Using (A) | Attitude Toward Using | Attitude Toward Using | — (dropped in UTAUT) | — (dropped in UTAUT2) | Attitude Toward the Behavior | Attitude Toward AI | Affective Attitude, Cognitive Attitude, General Attitude, Favorability, Evaluation |
| **SE** (Self-Efficacy) | — (not in TAM) | Computer Self-Efficacy (CSE) | Computer Self-Efficacy (CSE) | — (absorbed into EE) | — (absorbed into EE) | Perceived Behavioral Control (partial) | AI Self-Efficacy, Digital Self-Efficacy | Computer Self-Efficacy, Technology Self-Efficacy, Confidence in Using AI, Personal Competence, Skill Perception |
| **TRU** (AI Trust) | — (not in TAM) | — (not in TAM2) | Trust in Technology (added post-TAM3) | — (not in UTAUT) | — (not in UTAUT2) | — (not in TPB) | **Trust in AI**, Algorithm Aversion (reverse), Trust in Automation | Trustworthiness, Reliability Perception, Credibility, Dependability, Trust in System, Algorithm Trust, Institutional Trust in AI |
| **ANX** (AI Anxiety) | Computer Anxiety (reverse of PEOU in some models) | Computer Anxiety | Computer Anxiety | Anxiety (moderator in UTAUT) | — (dropped in UTAUT2) | — (not in TPB) | **AI Anxiety**, Technophobia, Algorithm Aversion, Fear of AI | Technology Anxiety, Cybernetic Anxiety, Emotional Resistance, Apprehension, Fear of Replacement, Job Insecurity (AI-related) |
| **TRA** (AI Transparency) | — (not in TAM) | — (not in TAM2) | — (not in TAM3) | — (not in UTAUT) | — (not in UTAUT2) | — (not in TPB) | **AI Transparency**, Explainability, Interpretability, Algorithmic Transparency | Explainable AI (XAI), Understandability, Process Transparency, Black-Box Perception (reverse), Algorithmic Accountability |
| **AUT** (Perceived AI Autonomy) | — (not in TAM) | — (not in TAM2) | — (not in TAM3) | — (not in UTAUT) | — (not in UTAUT2) | — (not in TPB) | **AI Autonomy**, Perceived Intelligence, AI Agency, Automation Level | Anthropomorphism (related), Machine Agency, AI Decision Authority, Level of Automation, Human-in-the-Loop (reverse) |

---

## Detailed Construct Definitions

### 1. Performance Expectancy (PE)

**Standardized Definition:**
> The degree to which an individual believes that using AI technology will help them attain gains in job performance, productivity, or goal achievement.

**TAM Origin:** Perceived Usefulness (PU) — "the degree to which a person believes that using a particular system would enhance his or her job performance" (Davis, 1989, p. 320).

**UTAUT Definition:** "The degree to which an individual believes that using the system will help him or her to attain gains in job performance" (Venkatesh et al., 2003, p. 447).

**Key Scale Items (Representative):**
- "Using AI would improve my performance in my job/tasks."
- "Using AI would increase my productivity."
- "Using AI would enhance my effectiveness."
- "I would find AI useful in my work/daily life."
- "Using AI enables me to accomplish tasks more quickly."

**Education-Specific Examples:**
- "Using AI improves my learning effectiveness."
- "AI helps me understand course material better."
- "AI tutoring systems enhance my academic performance."
- "AI writing assistants improve my assignment quality."
- "AI tools make me more productive in teaching."
- "Using AI enhances my instructional effectiveness."

**Common Measurement Instruments:**
- Davis (1989) PU scale (6 items)
- Venkatesh et al. (2003) PE scale (4 items)
- Taylor & Todd (1995) Perceived Usefulness (adapted)

**Mapping Decision Rules:**

| Study Label | Map to PE? | Confidence | Notes |
|-------------|-----------|------------|-------|
| Perceived Usefulness, PU | Yes | Exact | Direct synonym |
| Performance Expectancy, PE | Yes | Exact | UTAUT label |
| Outcome Expectations | Yes | High | Bandura's SCT; similar construct |
| Relative Advantage | Yes | High | IDT construct; conceptually equivalent |
| Job Fit | Yes | Moderate | TTF construct; emphasizes task match |
| Expected Performance | Yes | High | Alternative phrasing |
| Productivity Gain | Yes | Moderate | Outcome-focused; narrower scope |
| Efficiency | Partial | Low | May conflate with EE; review items |

**Ambiguous Cases:**
- **"Perceived Value"**: May include cost considerations (UTAUT2's Price Value). Review items to distinguish from pure performance.
- **"Perceived Benefits"**: Broader than PE; may include hedonic/social benefits. Code as PE only if items focus on performance/productivity.

---

### 2. Effort Expectancy (EE)

**Standardized Definition:**
> The degree of ease associated with using AI technology; the perceived cognitive or physical effort required to interact with the system.

**TAM Origin:** Perceived Ease of Use (PEOU) — "the degree to which a person believes that using a particular system would be free of effort" (Davis, 1989, p. 320).

**UTAUT Definition:** "The degree of ease associated with the use of the system" (Venkatesh et al., 2003, p. 450).

**Key Scale Items:**
- "Learning to use AI would be easy for me."
- "I would find AI easy to use."
- "My interaction with AI would be clear and understandable."
- "It would be easy for me to become skillful at using AI."
- "I would find AI flexible to interact with."

**Education-Specific Examples:**
- "Educational AI tools are easy to learn."
- "AI learning platforms are simple to navigate."
- "Interacting with AI tutoring systems is straightforward."
- "Using AI for coursework requires little effort."

**Common Measurement Instruments:**
- Davis (1989) PEOU scale (6 items)
- Venkatesh et al. (2003) EE scale (4 items)
- Moore & Benbasat (1991) Ease of Use (IDT adaptation)

**Mapping Decision Rules:**

| Study Label | Map to EE? | Confidence | Notes |
|-------------|-----------|------------|-------|
| Perceived Ease of Use, PEOU | Yes | Exact | Direct synonym |
| Effort Expectancy, EE | Yes | Exact | UTAUT label |
| Complexity (reverse-coded) | Yes | High | IDT construct; inverse of EE |
| Ease of Learning | Yes | High | Subset of EE |
| User-Friendliness | Yes | High | Common alternative |
| Usability | Yes | Moderate | Broader HCI construct; if items match EE definition |
| Cognitive Effort (reverse) | Yes | Moderate | Effort-focused; ensure reverse-coded |
| Simplicity | Yes | High | Positive framing of low complexity |

**Ambiguous Cases:**
- **"Learnability"**: Acceptable if items focus on ease of learning (subset of EE).
- **"System Quality"**: Too broad; includes reliability, functionality. Do NOT map to EE unless items specifically measure ease of use.

---

### 3. Social Influence (SI)

**Standardized Definition:**
> The degree to which an individual perceives that important others (peers, supervisors, family) believe they should use AI technology.

**TAM/TAM2 Origin:** Not in original TAM. Introduced in TAM2 as Subjective Norm (SN) and Image.

**UTAUT Definition:** "The degree to which an individual perceives that important others believe he or she should use the new system" (Venkatesh et al., 2003, p. 451).

**Key Scale Items:**
- "People who are important to me think I should use AI."
- "People who influence my behavior think I should use AI."
- "My supervisor/peers have been helpful in using AI."
- "Using AI improves my image/status in my organization."

**Education-Specific Examples:**
- "My classmates think I should use AI for learning."
- "My instructor recommends using AI tools."
- "Peer AI use in class influences my adoption."
- "Other teachers encourage AI use in instruction."
- "Students expect instructors to use AI tools."

**Common Measurement Instruments:**
- Venkatesh et al. (2003) SI scale (4 items)
- Ajzen (1991) Subjective Norm (TPB; 2-3 items)
- Taylor & Todd (1995) Subjective Norm

**Mapping Decision Rules:**

| Study Label | Map to SI? | Confidence | Notes |
|-------------|-----------|------------|-------|
| Social Influence, SI | Yes | Exact | UTAUT label |
| Subjective Norm, SN | Yes | Exact | TPB/TAM2 label |
| Image | Yes | High | TAM2 construct; status/prestige from use |
| Social Norms | Yes | High | Alternative phrasing |
| Peer Influence | Yes | High | Subset of SI |
| Normative Beliefs | Yes | High | TPB antecedent to SN |
| Social Pressure | Yes | High | Colloquial phrasing |
| Social Factors | Yes | Moderate | Ensure items match SI definition |

**Ambiguous Cases:**
- **"Social Support"**: May conflate with Facilitating Conditions (help/resources). Code as SI only if items measure normative pressure, not tangible support.
- **"Organizational Culture"**: Too broad. Code as SI only if items specifically measure peer/supervisor expectations.

---

### 4. Facilitating Conditions (FC)

**Standardized Definition:**
> The degree to which an individual believes that organizational and technical infrastructure exists to support use of AI technology.

**UTAUT Origin:** "The degree to which an individual believes that an organizational and technical infrastructure exists to support use of the system" (Venkatesh et al., 2003, p. 453).

**Key Scale Items:**
- "I have the resources necessary to use AI."
- "I have the knowledge necessary to use AI."
- "AI is compatible with other systems I use."
- "A specific person (or group) is available for assistance with AI difficulties."
- "Technical support is available when I have problems with AI."

**Education-Specific Examples:**
- "My school/university provides AI infrastructure support."
- "Institutional AI resources are available for learning."
- "The LMS supports AI tool integration."
- "IT support helps with AI technology issues."
- "My institution has adequate AI technology resources."
- "School technology resources support AI use."

**Common Measurement Instruments:**
- Venkatesh et al. (2003) FC scale (4 items)
- Taylor & Todd (1995) Resource Facilitating Conditions, Technology Facilitating Conditions
- Triandis (1980) Facilitating Conditions

**Mapping Decision Rules:**

| Study Label | Map to FC? | Confidence | Notes |
|-------------|-----------|------------|-------|
| Facilitating Conditions, FC | Yes | Exact | UTAUT label |
| Resource Availability | Yes | High | Subset of FC |
| Technical Support | Yes | High | Organizational support aspect |
| Organizational Support | Yes | Moderate | Ensure items focus on resources/infrastructure, not normative pressure (SI) |
| Compatibility | Yes | High | System integration aspect |
| Training | Yes | Moderate | Resource aspect; may overlap with SE if self-assessment |
| Infrastructure Support | Yes | High | Alternative phrasing |

**Ambiguous Cases:**
- **"Perceived Behavioral Control" (PBC)**: TPB construct that conflates internal (self-efficacy) and external (facilitating conditions) control. Disaggregate: internal → SE, external → FC.
- **"Management Support"**: Code as FC if items mention resource provision. Code as SI if items mention normative expectations.

---

### 5. Behavioral Intention (BI)

**Standardized Definition:**
> An individual's stated willingness or plan to use AI technology in the future; the subjective probability of performing the behavior.

**TAM/UTAUT Definition:** Intention to use the technology (Davis, 1989; Venkatesh et al., 2003).

**Key Scale Items:**
- "I intend to use AI in the next [timeframe]."
- "I predict I would use AI in the next [timeframe]."
- "I plan to use AI in the next [timeframe]."
- "I am willing to use AI."
- "I will recommend AI to others."

**Common Measurement Instruments:**
- Davis (1989) BI scale (2 items)
- Venkatesh et al. (2003) BI scale (3 items)
- Ajzen (1991) Intention (TPB; 2-3 items)

**Mapping Decision Rules:**

| Study Label | Map to BI? | Confidence | Notes |
|-------------|-----------|------------|-------|
| Behavioral Intention, BI | Yes | Exact | Standard label |
| Intention to Use | Yes | Exact | Direct synonym |
| Adoption Intention | Yes | High | Pre-adoption stage |
| Continuance Intention | Yes | High | Post-adoption stage; conceptually similar |
| Usage Intention | Yes | Exact | Alternative phrasing |
| Willingness to Use | Yes | High | Similar construct; less volitional than intention |
| Acceptance Intention | Yes | High | Alternative phrasing |

**Ambiguous Cases:**
- **"Recommendation Intention"**: Code as separate construct if focal outcome. Code as BI if single item within BI scale.
- **"Purchase Intention"**: Consumer context; acceptable as BI if AI product/service.

---

### 6. Use Behavior (UB)

**Standardized Definition:**
> Actual use of AI technology; observable behavior, frequency, or duration of usage.

**TAM/UTAUT Definition:** Objective or self-reported actual use (Davis, 1989; Venkatesh et al., 2003).

**Key Measurement Approaches:**
- **Objective:** Log data, usage analytics, system records
- **Self-report:** Frequency (times per day/week), Duration (hours per week), Breadth (number of features used)

**Common Measurement Instruments:**
- Venkatesh et al. (2003) Use Behavior (self-report or objective)
- Davis (1989) Actual System Use (self-report)
- Log-based metrics (objective)

**Mapping Decision Rules:**

| Study Label | Map to UB? | Confidence | Notes |
|-------------|-----------|------------|-------|
| Use Behavior, UB | Yes | Exact | UTAUT label |
| Actual Use | Yes | Exact | Direct synonym |
| System Use | Yes | Exact | TAM label |
| Usage Frequency | Yes | High | Operationalization of use |
| Usage Duration | Yes | High | Operationalization of use |
| Adoption Behavior | Yes | High | Binary (adopted vs. not); acceptable |
| Continuance Use | Yes | High | Post-adoption use; conceptually similar |
| Usage | Yes | Exact | Shorthand |

**Ambiguous Cases:**
- **"System Utilization"**: Acceptable if measured as frequency/duration. May confound with "effective use" (using features appropriately); prefer simple use metrics.
- **"Feature Usage"**: Breadth of use; acceptable if aggregated to overall usage construct.

---

### 7. Attitude (ATT)

**Standardized Definition:**
> An individual's overall affective and evaluative response to using AI technology; feelings of favorability or unfavorability.

**TAM Origin:** Attitude Toward Using — "an individual's positive or negative feelings about performing the target behavior" (Davis, 1989, adapted from Fishbein & Ajzen, 1975).

**Key Scale Items:**
- "Using AI is a good/bad idea."
- "Using AI is pleasant/unpleasant."
- "Using AI is favorable/unfavorable."
- "I like/dislike using AI."
- "Using AI is beneficial/harmful."

**Common Measurement Instruments:**
- Davis (1989) Attitude scale (semantic differential)
- Ajzen (1991) Attitude (TPB)
- Taylor & Todd (1995) Attitude

**Mapping Decision Rules:**

| Study Label | Map to ATT? | Confidence | Notes |
|-------------|-----------|------------|-------|
| Attitude, ATT | Yes | Exact | Standard label |
| Attitude Toward Using | Yes | Exact | TAM label |
| Attitude Toward AI | Yes | Exact | AI-specific phrasing |
| Affective Attitude | Yes | High | Feeling component; acceptable |
| Cognitive Attitude | Yes | Moderate | Belief component; may overlap with PE/EE |
| General Attitude | Yes | High | Overall evaluation |
| Favorability | Yes | High | Synonym |

**Ambiguous Cases:**
- **"Satisfaction"**: Post-usage evaluation; distinct from attitude (pre/during-usage affect). Code separately if available; otherwise may approximate ATT in post-adoption studies.
- **"Liking"**: Acceptable as ATT if measured as overall evaluation. If specific to aesthetics (interface design), code separately.

---

### 8. Self-Efficacy (SE)

**Standardized Definition:**
> An individual's judgment of their capability to use AI technology; confidence in ability to perform AI-related tasks.

**TAM2/TAM3 Origin:** Computer Self-Efficacy (CSE) — "judgment of one's ability to use a computer" (Compeau & Higgins, 1995; added to TAM2).

**SCT Origin:** Bandura (1986) — perceived capability to perform behavior.

**Key Scale Items:**
- "I could complete tasks using AI if there was no one around to help me."
- "I could complete tasks using AI if I had seen someone else use it before trying it myself."
- "I could complete tasks using AI if I had a lot of time to complete the task."
- "I am confident in my ability to use AI."

**Education-Specific Examples:**
- "I am confident in my academic AI self-efficacy."
- "I can use AI learning tools effectively on my own."
- "I have the AI literacy skills needed for coursework."
- "I am confident using AI for teaching tasks."

**Common Measurement Instruments:**
- Compeau & Higgins (1995) Computer Self-Efficacy (10 items)
- Venkatesh et al. (2003) Self-Efficacy (removed from final UTAUT but measured in development)
- Bandura (2006) Guide for Constructing Self-Efficacy Scales

**Mapping Decision Rules:**

| Study Label | Map to SE? | Confidence | Notes |
|-------------|-----------|------------|-------|
| Self-Efficacy, SE | Yes | Exact | Standard label |
| Computer Self-Efficacy, CSE | Yes | Exact | TAM2/3 label |
| AI Self-Efficacy | Yes | Exact | AI-specific phrasing |
| Technology Self-Efficacy | Yes | High | Broader construct; acceptable |
| Digital Self-Efficacy | Yes | High | Digital literacy context |
| Confidence in Using AI | Yes | High | Layperson phrasing |
| Personal Competence | Yes | Moderate | Ensure items focus on capability judgment, not past performance |

**Ambiguous Cases:**
- **"Perceived Behavioral Control" (PBC)**: TPB construct conflating internal (efficacy) and external (facilitating conditions) control. Disaggregate items into SE and FC.
- **"Skill"**: Actual ability (objective). SE is *perceived* capability (subjective). Code as SE only if self-assessment.

---

### 9. AI Trust (TRU)

**Standardized Definition:**
> The willingness to be vulnerable to AI actions based on positive expectations of AI intentions, abilities, and integrity; belief that AI will perform as expected without harmful consequences.

**Theoretical Basis:**
- Mayer et al. (1995) Organizational Trust (ability, benevolence, integrity)
- McKnight et al. (2002) Trust in Technology
- Choung et al. (2023) Trust in AI

**Key Scale Items:**
- "I trust AI to perform tasks accurately."
- "I believe AI is reliable."
- "AI has the ability to do what I need it to do."
- "I believe AI will not misuse my data."
- "I feel secure using AI."

**Education-Specific Examples:**
- "I trust AI tutoring systems to provide accurate feedback."
- "I trust AI grading to be fair and accurate."
- "AI assessment tools are trustworthy."
- "I trust AI writing feedback for learning."
- "Educational AI protects student data privacy."

**Common Measurement Instruments:**
- Choung et al. (2023) Trust in AI scale
- McKnight et al. (2002) Trust in Technology (adapted)
- Jian et al. (2000) Trust in Automation scale
- Körber (2018) Trust in Automation (short version)

**Mapping Decision Rules:**

| Study Label | Map to TRU? | Confidence | Notes |
|-------------|-----------|------------|-------|
| Trust in AI | Yes | Exact | AI-specific label |
| AI Trust | Yes | Exact | Shorthand |
| Trust in Automation | Yes | High | Automation literature; conceptually similar |
| Trustworthiness | Yes | High | Object attribute; if measured from user perspective |
| Algorithm Trust | Yes | Exact | Algorithm aversion literature |
| Reliability Perception | Yes | Moderate | Ability dimension of trust; acceptable if part of multi-dimensional trust |
| Credibility | Yes | Moderate | Information quality context; review items |
| Dependability | Yes | High | Synonym for reliability dimension |

**Ambiguous Cases:**
- **"Perceived Risk" (reverse)**: Risk is distinct from trust (outcome vs. belief). Code separately if available; may correlate negatively with TRU but not identical.
- **"Privacy Concern" (reverse)**: Specific to data protection; distinct from general trust. Code separately.
- **"Algorithm Aversion"**: Resistance to algorithmic advice; inverse of algorithm trust. Code as TRU (reverse-coded) if items measure trust/distrust.

---

### 10. AI Anxiety (ANX)

**Standardized Definition:**
> Negative affective responses (fear, apprehension, worry) evoked by using or thinking about using AI technology; emotional discomfort or stress associated with AI.

**Theoretical Basis:**
- Computer Anxiety (Venkatesh et al., 2003 — UTAUT moderator)
- Technophobia (Brosnan, 1998)
- AI Anxiety (Sindermann et al., 2021)

**Key Scale Items:**
- "AI makes me feel uncomfortable."
- "I feel apprehensive about using AI."
- "Working with AI makes me nervous."
- "AI is somewhat intimidating to me."
- "I worry about losing my job to AI." (job replacement anxiety)

**Education-Specific Examples:**
- "I worry about AI academic integrity issues (plagiarism)."
- "AI use in coursework makes me anxious about cheating."
- "I fear AI over-reliance will hurt my learning."
- "Using AI for assignments causes academic integrity anxiety."
- "AI grading makes me nervous about fairness."
- "I worry AI will replace human teaching."

**Common Measurement Instruments:**
- Sindermann et al. (2021) AI Anxiety scale
- Venkatesh et al. (2003) Computer Anxiety (adapted)
- Heinssen et al. (1987) Computer Anxiety Rating Scale (CARS)

**Mapping Decision Rules:**

| Study Label | Map to ANX? | Confidence | Notes |
|-------------|-----------|------------|-------|
| AI Anxiety | Yes | Exact | AI-specific label |
| Computer Anxiety | Yes | High | General technology anxiety; acceptable |
| Technology Anxiety | Yes | High | Broader construct; acceptable |
| Technophobia | Yes | High | Trait-level fear of technology |
| Apprehension | Yes | High | Synonym |
| Fear of AI | Yes | Exact | Layperson phrasing |
| Cybernetic Anxiety | Yes | Moderate | Automation anxiety; review items |
| Emotional Resistance | Yes | Moderate | Ensure items measure anxiety, not general resistance (which may be cognitive) |

**Ambiguous Cases:**
- **"Job Insecurity"**: If AI-related, can map to ANX (job replacement anxiety dimension). If general economic insecurity, code separately.
- **"Worry"**: Acceptable as ANX if worry is about AI use/consequences. If worry about learning (lack of confidence), may overlap with low SE.
- **"Stress"**: General construct; code as ANX only if items specifically mention AI as stressor.

---

### 11. AI Transparency (TRA)

**Standardized Definition:**
> The degree to which an individual perceives that AI decision-making processes are understandable, explainable, and open to inspection; clarity about how AI arrives at outputs.

**Theoretical Basis:**
- Explainable AI (XAI) literature (Arrieta et al., 2020)
- Algorithmic Transparency (Felzmann et al., 2019)
- AI Interpretability (Doshi-Velez & Kim, 2017)

**Key Scale Items:**
- "I understand how AI arrives at its decisions."
- "AI provides explanations for its outputs."
- "The AI's decision-making process is transparent to me."
- "I can see why AI recommends certain actions."
- "AI's inner workings are clear."

**Education-Specific Examples:**
- "AI grading criteria are transparent and explainable."
- "Educational AI explains its recommendations clearly."
- "I understand how AI tutoring systems generate feedback."
- "AI learning platforms show how they personalize content."

**Common Measurement Instruments:**
- Limited validated scales; often ad-hoc items in XAI studies
- Shin (2021) Algorithm Transparency scale
- Felzmann et al. (2019) Transparency items

**Mapping Decision Rules:**

| Study Label | Map to TRA? | Confidence | Notes |
|-------------|-----------|------------|-------|
| AI Transparency | Yes | Exact | AI-specific label |
| Algorithmic Transparency | Yes | Exact | Algorithm-focused phrasing |
| Explainability | Yes | High | Ability to explain; user perception of |
| Interpretability | Yes | High | Ability to interpret; user perception of |
| Understandability | Yes | High | User comprehension of AI processes |
| Explainable AI (XAI) | Yes | High | Technical term; user perception |
| Process Transparency | Yes | High | Alternative phrasing |
| Algorithmic Accountability | Yes | Moderate | Broader construct including transparency + responsibility; review items |

**Ambiguous Cases:**
- **"Information Quality"**: Too broad; includes accuracy, completeness. Code as TRA only if items specifically measure process transparency.
- **"Black-Box Perception" (reverse)**: Inverse of transparency; acceptable as TRA (reverse-coded).
- **"Output Justification"**: Specific to explanation provision; subset of TRA; acceptable.

---

### 12. Perceived AI Autonomy (AUT)

**Standardized Definition:**
> The degree to which an individual perceives that AI operates independently, makes decisions without human intervention, and exhibits agency or self-directed behavior.

**Theoretical Basis:**
- Levels of Automation (Parasuraman et al., 2000)
- Anthropomorphism (Waytz et al., 2010) — attributing human-like agency to AI
- Perceived Intelligence (Moon, 2000)

**Key Scale Items:**
- "The AI operates independently without human control."
- "The AI makes decisions on its own."
- "The AI has a mind of its own."
- "The AI acts autonomously."
- "I perceive the AI as having its own intentions."

**Common Measurement Instruments:**
- Limited validated scales; often ad-hoc
- Anthropomorphism measures (Waytz et al., 2010; adapted)
- Moon (2000) Perceived Intelligence items (adapted)
- Automation level perception (Parasuraman et al., 2000; adapted)

**Mapping Decision Rules:**

| Study Label | Map to AUT? | Confidence | Notes |
|-------------|-----------|------------|-------|
| Perceived AI Autonomy | Yes | Exact | Standard label |
| AI Autonomy | Yes | Exact | Shorthand |
| Perceived Intelligence | Yes | High | AI capability to act independently |
| AI Agency | Yes | Exact | Anthropomorphic agency attribution |
| Automation Level | Yes | High | Degree of human-out-of-loop operation |
| Anthropomorphism | Yes | Moderate | Broader construct; includes autonomy + human-likeness. If items focus on agency/independence, code as AUT |
| Machine Agency | Yes | Exact | Alternative phrasing |

**Ambiguous Cases:**
- **"Control" (reverse)**: User control over AI is inverse of autonomy. Code as AUT (reverse) if items measure perceived AI independence vs. user control.
- **"Human-in-the-Loop" (reverse)**: System design feature; if measured as user perception, code as AUT (reverse).
- **"Decision Authority"**: Organizational construct; code as AUT only if items measure perception of AI's independent decision-making.

---

## Construct Harmonization Decision Rules

### General Principles

1. **Prioritize conceptual equivalence over label similarity**: Focus on item content and theoretical definition, not just construct name.

2. **When in doubt, flag for expert review**: Low-confidence mappings should be reviewed by senior author or domain expert.

3. **Avoid double-coding**: Each study-specific construct should map to ONE standardized construct. If items span multiple constructs, disaggregate.

4. **Document deviations**: Note any study-specific operationalizations that deviate from standard definitions.

### Confidence Ratings

- **Exact**: Identical label + definition, validated scale from canonical source (e.g., Davis, 1989 PU = PE)
- **High**: Synonymous label, matching definition, similar items (>80% item overlap)
- **Moderate**: Different label but matching definition, partial item overlap (50-80%)
- **Low**: Ambiguous definition, <50% item overlap, or conflated constructs requiring judgment

### Handling Edge Cases

**Case 1: Conflated Constructs (e.g., PBC in TPB)**
- **Solution**: Disaggregate based on item content
  - Items about personal capability → SE
  - Items about resources/support → FC
  - If impossible to disaggregate, flag and exclude from analysis

**Case 2: Multi-Dimensional Constructs**
- **Solution**: If study reports subscales (e.g., Trust = Ability + Benevolence + Integrity), use overall/composite score as TRU
- If only subscales reported without composite, use the most relevant dimension (e.g., Ability for TRU) and document in notes

**Case 3: Related but Distinct Constructs**
- **Satisfaction vs. ATT**: Satisfaction is post-usage evaluation; ATT is pre/during-usage affect. Keep separate.
- **Perceived Risk vs. TRU**: Risk is outcome probability; trust is belief in AI integrity. Keep separate.
- **Enjoyment/Hedonic Motivation vs. ATT**: Hedonic motivation (UTAUT2) is specific to intrinsic enjoyment; ATT is broader evaluation. If study uses both, keep separate. If only hedonic motivation available, acceptable as ATT proxy in post-hoc analysis.

**Case 4: Missing Constructs**
- If study omits a construct entirely, code as "NA" (not measured)
- Do NOT impute zero or mean values
- Do NOT code related-but-distinct constructs as substitutes without strong justification

---

## Validation Checklist

Use this checklist for each construct mapping:

- [ ] Study-specific label documented verbatim
- [ ] Conceptual definition from study extracted (if available)
- [ ] Sample items reviewed (minimum 3 items)
- [ ] Standardized construct assigned
- [ ] Confidence rating determined (exact/high/moderate/low)
- [ ] Measurement source cited (e.g., Davis, 1989; Venkatesh et al., 2003)
- [ ] Reliability (α) recorded
- [ ] Low-confidence mappings flagged for expert review
- [ ] Notes added for deviations or ambiguities

---

## References

- Ajzen, I. (1991). The theory of planned behavior. *Organizational Behavior and Human Decision Processes*, 50(2), 179-211.
- Arrieta, A. B., et al. (2020). Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. *Information Fusion*, 58, 82-115.
- Bandura, A. (1986). *Social foundations of thought and action: A social cognitive theory*. Prentice-Hall.
- Brosnan, M. J. (1998). The impact of computer anxiety and self-efficacy upon performance. *Journal of Computer Assisted Learning*, 14(3), 223-234.
- Choung, H., David, P., & Ross, A. (2023). Trust in AI and its role in the acceptance of AI technologies. *International Journal of Human-Computer Interaction*, 39(9), 1727-1739.
- Compeau, D. R., & Higgins, C. A. (1995). Computer self-efficacy: Development of a measure and initial test. *MIS Quarterly*, 19(2), 189-211.
- Davis, F. D. (1989). Perceived usefulness, perceived ease of use, and user acceptance of information technology. *MIS Quarterly*, 13(3), 319-340.
- Doshi-Velez, F., & Kim, B. (2017). Towards a rigorous science of interpretable machine learning. *arXiv preprint arXiv:1702.08608*.
- Felzmann, H., et al. (2019). Transparency you can trust: Transparency requirements for artificial intelligence between legal norms and contextual concerns. *Big Data & Society*, 6(1), 1-14.
- Fishbein, M., & Ajzen, I. (1975). *Belief, attitude, intention, and behavior: An introduction to theory and research*. Addison-Wesley.
- Heinssen, R. K., Glass, C. R., & Knight, L. A. (1987). Assessing computer anxiety: Development and validation of the Computer Anxiety Rating Scale. *Computers in Human Behavior*, 3(1), 49-59.
- Jian, J.-Y., Bisantz, A. M., & Drury, C. G. (2000). Foundations for an empirically determined scale of trust in automated systems. *International Journal of Cognitive Ergonomics*, 4(1), 53-71.
- Körber, M. (2018). Theoretical considerations and development of a questionnaire to measure trust in automation. In *Proceedings of the 20th Congress of the International Ergonomics Association* (pp. 13-30). Springer.
- Mayer, R. C., Davis, J. H., & Schoorman, F. D. (1995). An integrative model of organizational trust. *Academy of Management Review*, 20(3), 709-734.
- McKnight, D. H., Choudhury, V., & Kacmar, C. (2002). Developing and validating trust measures for e-commerce: An integrative typology. *Information Systems Research*, 13(3), 334-359.
- Moon, Y. (2000). Intimate exchanges: Using computers to elicit self-disclosure from consumers. *Journal of Consumer Research*, 26(4), 323-339.
- Moore, G. C., & Benbasat, I. (1991). Development of an instrument to measure the perceptions of adopting an information technology innovation. *Information Systems Research*, 2(3), 192-222.
- Parasuraman, R., Sheridan, T. B., & Wickens, C. D. (2000). A model for types and levels of human interaction with automation. *IEEE Transactions on Systems, Man, and Cybernetics—Part A: Systems and Humans*, 30(3), 286-297.
- Shin, D. (2021). The effects of explainability and causability on perception, trust, and acceptance: Implications for explainable AI. *International Journal of Human-Computer Interaction*, 37(10), 917-931.
- Sindermann, C., et al. (2021). Assessing the attitude towards artificial intelligence: Introduction of a short measure in German, Chinese, and English language. *KI-Künstliche Intelligenz*, 35(1), 109-118.
- Taylor, S., & Todd, P. A. (1995). Understanding information technology usage: A test of competing models. *Information Systems Research*, 6(2), 144-176.
- Triandis, H. C. (1980). Values, attitudes, and interpersonal behavior. In H. E. Howe & M. M. Page (Eds.), *Nebraska Symposium on Motivation* (Vol. 27, pp. 195-259). University of Nebraska Press.
- Venkatesh, V., & Bala, H. (2008). Technology Acceptance Model 3 and a research agenda on interventions. *Decision Sciences*, 39(2), 273-315.
- Venkatesh, V., & Davis, F. D. (2000). A theoretical extension of the Technology Acceptance Model: Four longitudinal field studies. *Management Science*, 46(2), 186-204.
- Venkatesh, V., Morris, M. G., Davis, G. B., & Davis, F. D. (2003). User acceptance of information technology: Toward a unified view. *MIS Quarterly*, 27(3), 425-478.
- Venkatesh, V., Thong, J. Y., & Xu, X. (2012). Consumer acceptance and use of information technology: Extending the Unified Theory of Acceptance and Use of Technology. *MIS Quarterly*, 36(1), 157-178.
- Waytz, A., Cacioppo, J., & Epley, N. (2010). Who sees human? The stability and importance of individual differences in anthropomorphism. *Perspectives on Psychological Science*, 5(3), 219-232.

---

**End of Construct Crosswalk**

**Version:** 1.0.0
**Date:** 2026-02-16
